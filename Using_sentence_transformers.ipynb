{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140dc93d",
   "metadata": {},
   "source": [
    "Two ideas to work on\n",
    "1) Train sentence transformers\n",
    "2) Find a legal courpus for training for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd32732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "02c8a6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake-nltk\n",
      "  Using cached rake_nltk-1.0.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: nltk in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from rake-nltk) (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from nltk->rake-nltk) (4.61.1)\n",
      "Requirement already satisfied: regex in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from nltk->rake-nltk) (2020.11.13)\n",
      "Requirement already satisfied: click in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from nltk->rake-nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from nltk->rake-nltk) (1.0.1)\n",
      "Installing collected packages: rake-nltk\n",
      "Successfully installed rake-nltk-1.0.4Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "50e37130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensimNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading gensim-4.1.0-cp37-cp37m-win_amd64.whl (23.9 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from gensim) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Collecting Cython==0.29.23\n",
      "  Downloading Cython-0.29.23-cp37-cp37m-win_amd64.whl (1.6 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "Successfully installed Cython-0.29.23 gensim-4.1.0 smart-open-5.2.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b55c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r'semantic-segmentation-master\\semantic-segmentation-master\\data\\text\\2007_C_121.txt',sep='\\t',header=None)\n",
    "# df.columns = ['sentence','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc55480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_1.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_10.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_11.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_12.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_13.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_14.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_15.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_16.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_17.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_18.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_19.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_2.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_20.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_21.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_22.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_23.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_24.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_25.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_26.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_27.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_28.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_29.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_3.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_30.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_31.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_32.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_33.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_34.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_35.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_36.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_37.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_38.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_39.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_4.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_40.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_41.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_42.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_43.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_44.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_45.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_46.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_47.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_48.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_49.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_5.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_50.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_51.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_52.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_53.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_54.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_55.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_56.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_57.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_58.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_59.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_6.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_60.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_7.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_8.txt\n",
      "dataset\\Task1-20210907T155244Z-001\\Task1\\d_9.txt\n"
     ]
    }
   ],
   "source": [
    "folder = r'dataset\\Task1-20210907T155244Z-001\\Task1'\n",
    "df = None\n",
    "for f in os.listdir(folder):\n",
    "    filepath=os.path.join(folder,f)\n",
    "    print(filepath)\n",
    "    if df is None:\n",
    "        df = pd.read_csv(filepath,sep='\\t',header=None)\n",
    "    else:\n",
    "        new_df = pd.read_csv(filepath,sep='\\t',header=None)\n",
    "        df = pd.concat([df,new_df])\n",
    "\n",
    "df.columns = ['sentence','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee76ad78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One Lakshminarayana Iyer, a Hindu Brahmin, who...</td>\n",
       "      <td>Facts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ramalakshmi had married the plaintiff and had ...</td>\n",
       "      <td>Facts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They were all alive in December, 1924, when La...</td>\n",
       "      <td>Facts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Before his death he executed a will on 16th No...</td>\n",
       "      <td>Ratio of the decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By this will he gave the following directions ...</td>\n",
       "      <td>Facts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>In these circumstances it would be fair and pr...</td>\n",
       "      <td>Ratio of the decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>If and when he prefers a claim to this particu...</td>\n",
       "      <td>Ratio of the decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>In this view of the matter,we feel that the Hi...</td>\n",
       "      <td>Ratio of the decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>The appeal is,therefore,dismissed with costs</td>\n",
       "      <td>Ruling by Present Court</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Appeal dismissed</td>\n",
       "      <td>Ruling by Present Court</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11053 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence                    label\n",
       "0   One Lakshminarayana Iyer, a Hindu Brahmin, who...                    Facts\n",
       "1   Ramalakshmi had married the plaintiff and had ...                    Facts\n",
       "2   They were all alive in December, 1924, when La...                    Facts\n",
       "3   Before his death he executed a will on 16th No...    Ratio of the decision\n",
       "4   By this will he gave the following directions ...                    Facts\n",
       "..                                                ...                      ...\n",
       "74  In these circumstances it would be fair and pr...    Ratio of the decision\n",
       "75  If and when he prefers a claim to this particu...    Ratio of the decision\n",
       "76  In this view of the matter,we feel that the Hi...    Ratio of the decision\n",
       "77       The appeal is,therefore,dismissed with costs  Ruling by Present Court\n",
       "78                                   Appeal dismissed  Ruling by Present Court\n",
       "\n",
       "[11053 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b98662b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords \n",
    "r = Rake()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a5b6f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_list = df['sentence'].tolist()\n",
    "text = \".\".join(map(str,sen_list))\n",
    "a=r.extract_keywords_from_text(text)\n",
    "b=r.get_ranked_phrases()\n",
    "c=r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c5712dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b5efe76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b564528c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One Lakshminarayana Iyer, a Hindu Brahmin, who owned considerable properties '\n",
      " 'in the Tirunelveli district, died on 13th December, 1924, leaving him '\n",
      " 'surviving a widow Ranganayaki, and a married daughter Ramalakshmi']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.sentence.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', str(sent)) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", str(sent)) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "17206a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['one', 'lakshminarayana', 'iyer', 'hindu', 'brahmin', 'who', 'owned', 'considerable', 'properties', 'in', 'the', 'tirunelveli', 'district', 'died', 'on', 'th', 'december', 'leaving', 'him', 'surviving', 'widow', 'ranganayaki', 'and', 'married', 'daughter', 'ramalakshmi']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "015ed424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'lakshminarayana', 'iyer', 'hindu', 'brahmin', 'who', 'owned', 'considerable', 'properties', 'in', 'the', 'tirunelveli', 'district', 'died', 'on', 'th_december', 'leaving', 'him', 'surviving', 'widow', 'ranganayaki', 'and', 'married', 'daughter', 'ramalakshmi']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=50)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "33de81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "13af87d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['own', 'considerable', 'property', 'district', 'die', 'leave', 'survive', 'widow', 'married', 'daughter', 'ramalakshmi']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d72197d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2028fcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('considerable', 1),\n",
       "  ('daughter', 1),\n",
       "  ('die', 1),\n",
       "  ('district', 1),\n",
       "  ('leave', 1),\n",
       "  ('married', 1),\n",
       "  ('own', 1),\n",
       "  ('property', 1),\n",
       "  ('ramalakshmi', 1),\n",
       "  ('survive', 1),\n",
       "  ('widow', 1)]]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ee9ebd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7ec3e496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.067*\"right\" + 0.040*\"may\" + 0.037*\"also\" + 0.033*\"give\" + 0.029*\"person\" '\n",
      "  '+ 0.028*\"provide\" + 0.028*\"officer\" + 0.028*\"therefore\" + 0.027*\"take\" + '\n",
      "  '0.026*\"law\"'),\n",
      " (1,\n",
      "  '0.055*\"article\" + 0.029*\"basis\" + 0.027*\"proprietary\" + 0.026*\"conclusion\" '\n",
      "  '+ 0.024*\"liable\" + 0.022*\"available\" + 0.021*\"note\" + 0.017*\"process\" + '\n",
      "  '0.017*\"company\" + 0.017*\"previous\"'),\n",
      " (2,\n",
      "  '0.076*\"time\" + 0.067*\"include\" + 0.046*\"low\" + 0.035*\"even\" + '\n",
      "  '0.029*\"product\" + 0.029*\"appoint\" + 0.028*\"allow\" + 0.027*\"first\" + '\n",
      "  '0.026*\"point\" + 0.022*\"examination\"'),\n",
      " (3,\n",
      "  '0.091*\"issue\" + 0.090*\"suit\" + 0.060*\"benefit\" + 0.042*\"thus\" + '\n",
      "  '0.037*\"concern\" + 0.033*\"declare\" + 0.033*\"vest\" + 0.031*\"nature\" + '\n",
      "  '0.028*\"stand\" + 0.026*\"submission\"'),\n",
      " (4,\n",
      "  '0.093*\"section\" + 0.068*\"case\" + 0.051*\"period\" + 0.036*\"language\" + '\n",
      "  '0.029*\"property\" + 0.028*\"accuse\" + 0.025*\"possession\" + 0.024*\"woman\" + '\n",
      "  '0.022*\"income\" + 0.022*\"sub\"'),\n",
      " (5,\n",
      "  '0.135*\"state\" + 0.088*\"respondent\" + 0.051*\"government\" + 0.047*\"fact\" + '\n",
      "  '0.032*\"village\" + 0.031*\"collector\" + 0.030*\"subject\" + 0.026*\"society\" + '\n",
      "  '0.025*\"amount\" + 0.024*\"proceeding\"'),\n",
      " (6,\n",
      "  '0.073*\"would\" + 0.064*\"provision\" + 0.061*\"compensation\" + 0.052*\"question\" '\n",
      "  '+ 0.045*\"rule\" + 0.031*\"inquiry\" + 0.025*\"follow\" + 0.024*\"contain\" + '\n",
      "  '0.024*\"term\" + 0.023*\"employee\"'),\n",
      " (7,\n",
      "  '0.072*\"act\" + 0.042*\"use\" + 0.042*\"pass\" + 0.038*\"member\" + 0.034*\"could\" + '\n",
      "  '0.031*\"purpose\" + 0.030*\"must\" + 0.028*\"raise\" + 0.026*\"party\" + '\n",
      "  '0.024*\"power\"'),\n",
      " (8,\n",
      "  '0.046*\"make\" + 0.045*\"land\" + 0.042*\"order\" + 0.038*\"date\" + 0.033*\"claim\" '\n",
      "  '+ 0.032*\"shall\" + 0.032*\"say\" + 0.030*\"patent\" + 0.029*\"appellant\" + '\n",
      "  '0.027*\"grant\"'),\n",
      " (9,\n",
      "  '0.170*\"court\" + 0.066*\"high\" + 0.060*\"appeal\" + 0.030*\"decision\" + '\n",
      "  '0.021*\"refer\" + 0.021*\"view\" + 0.019*\"stone\" + 0.017*\"adopt\" + '\n",
      "  '0.016*\"however\" + 0.016*\"receive\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "deea47ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\\test-aila-task1-2021-20210907T070626Z-001\\test-aila-task1-2021\\d1.txt\n",
      "test\\test-aila-task1-2021-20210907T070626Z-001\\test-aila-task1-2021\\d10.txt\n",
      "test\\test-aila-task1-2021-20210907T070626Z-001\\test-aila-task1-2021\\d2.txt\n",
      "test\\test-aila-task1-2021-20210907T070626Z-001\\test-aila-task1-2021\\d3.txt\n",
      "test\\test-aila-task1-2021-20210907T070626Z-001\\test-aila-task1-2021\\d4.txt\n",
      "test\\test-aila-task1-2021-20210907T070626Z-001\\test-aila-task1-2021\\d5.txt\n",
      "test\\test-aila-task1-2021-20210907T070626Z-001\\test-aila-task1-2021\\d6.txt\n",
      "test\\test-aila-task1-2021-20210907T070626Z-001\\test-aila-task1-2021\\d7.txt\n",
      "test\\test-aila-task1-2021-20210907T070626Z-001\\test-aila-task1-2021\\d8.txt\n",
      "test\\test-aila-task1-2021-20210907T070626Z-001\\test-aila-task1-2021\\d9.txt\n"
     ]
    }
   ],
   "source": [
    "test_folder = r'test\\test-aila-task1-2021-20210907T070626Z-001\\test-aila-task1-2021'\n",
    "test_df = None\n",
    "for f in os.listdir(test_folder):\n",
    "    filepath=os.path.join(test_folder,f)\n",
    "    print(filepath)\n",
    "    if test_df is None:\n",
    "        test_df = pd.read_csv(filepath,sep='\\t',header=None)\n",
    "    else:\n",
    "        new_df = pd.read_csv(filepath,sep='\\t',header=None)\n",
    "        test_df = pd.concat([test_df,new_df])\n",
    "\n",
    "# print(test_df)\n",
    "test_df.columns = ['sentence','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5bc7bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This appeal, by special leave, by the appellan...</td>\n",
       "      <td>Ratio of the decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The respondent was appointed by an order of th...</td>\n",
       "      <td>Facts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The category of the post to which she was appo...</td>\n",
       "      <td>Facts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By an order dated February 16, 1967 of the Dep...</td>\n",
       "      <td>Facts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The respondent filed Civil Rule no. 206 of 196...</td>\n",
       "      <td>Facts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Further, in various pleadings in the Title Sui...</td>\n",
       "      <td>Ratio of the decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>From these facts, it is clear that the respond...</td>\n",
       "      <td>Ratio of the decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>In view of the foregoing discussion, we do not...</td>\n",
       "      <td>Ruling by Present Court</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>However, the parties are left to bear their ow...</td>\n",
       "      <td>Ruling by Present Court</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Appeal dismissed</td>\n",
       "      <td>Ruling by Present Court</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>644 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence                    label\n",
       "0   This appeal, by special leave, by the appellan...    Ratio of the decision\n",
       "1   The respondent was appointed by an order of th...                    Facts\n",
       "2   The category of the post to which she was appo...                    Facts\n",
       "3   By an order dated February 16, 1967 of the Dep...                    Facts\n",
       "4   The respondent filed Civil Rule no. 206 of 196...                    Facts\n",
       "..                                                ...                      ...\n",
       "93  Further, in various pleadings in the Title Sui...    Ratio of the decision\n",
       "94  From these facts, it is clear that the respond...    Ratio of the decision\n",
       "95  In view of the foregoing discussion, we do not...  Ruling by Present Court\n",
       "96  However, the parties are left to bear their ow...  Ruling by Present Court\n",
       "97                                   Appeal dismissed  Ruling by Present Court\n",
       "\n",
       "[644 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e6d452e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e92a9e94474e038dc0b6fdb3bf84e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fb438027cc40f4a537f1659b1ecf48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb27c8a12314e7aa3f3f97330d6d429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef9dcb1d393405d8d8b77e2895fd207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prof = ProfileReport(df)\n",
    "prof.to_file(output_file='output.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3924f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.0.0.tar.gz (85 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from sentence-transformers) (4.61.1)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.9.0-cp37-cp37m-win_amd64.whl (222.0 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.10.0-cp37-cp37m-win_amd64.whl (920 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from sentence-transformers) (1.20.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\i515580\\appdata\\roaming\\python\\python37\\site-packages (from sentence-transformers) (0.24.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from sentence-transformers) (1.6.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from sentence-transformers) (3.5)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\i515580\\appdata\\roaming\\python\\python37\\site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.9)\n",
      "Requirement already satisfied: requests in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2020.11.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\i515580\\appdata\\roaming\\python\\python37\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.35)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: click in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from nltk->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.5)\n",
      "Requirement already satisfied: six in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from torchvision->sentence-transformers) (8.1.0)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-py3-none-any.whl size=126709 sha256=747ddf5c935b585cf5f96e7550de1a00363403e1c04d94572d98f0ee96127d3f\n",
      "  Stored in directory: c:\\users\\i515580\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\0f\\faafd427f705c4b012274ba60d9a91d75830306811e1355293\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: filelock, torch, tokenizers, huggingface-hub, transformers, torchvision, sentencepiece, sentence-transformers\n",
      "Successfully installed filelock-3.0.12 huggingface-hub-0.0.16 sentence-transformers-2.0.0 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.9.0 torchvision-0.10.0 transformers-4.10.0\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c87b053d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e953840299543589fdcac43ddd2fe18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3d3cb861ca4215a5f5e179e5abc857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab67e9019044cf1865441f0219a3fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f1f6800fb34eb88604429ffa91d4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3217040ef4a4b8896c915cd6eb5f3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41bb2cba2dd490f88f7ff391b45eb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6bff763f6b4c3ca7f69bf7e780c5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8c1928e6674dbc89612bc91c3280dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b379ca64f1c44ce4b8e78c4cfdc6b43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946039c84dae42d6897617ac07caeedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8190108201564af58618e065cad1f61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5fa27e1a4f4e18a2ab43fe61c602b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2697480ac1a4438d91890f351d15a962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8486e754cf4037a5e641b0ca6feea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c680fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['sentence'].tolist()\n",
    "sentence_embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3c160c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11053, 384)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "45ae270c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 384)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings = model.encode(test_df['sentence'].tolist())\n",
    "test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe844fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f6438b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()  \n",
    "integer_encoded = label_encoder.fit_transform(df['label'].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09a50a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Argument',\n",
       " 'Facts',\n",
       " 'Precedent',\n",
       " 'Ratio of the decision',\n",
       " 'Ruling by Lower Court',\n",
       " 'Ruling by Present Court',\n",
       " 'Statute']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ed19bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_integer_encoded = label_encoder.transform(test_df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b7e63bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)  \n",
    "oh_labels = enc.fit_transform(integer_encoded)  \n",
    "test_integer_encoded = test_integer_encoded.reshape(len(test_integer_encoded),1)\n",
    "test_oh_labels=enc.transform(test_integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2db80ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11053x7 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 11053 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e69c28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5c50efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I515580\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf.fit(sentence_embeddings, integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4362b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec = lin_clf.decision_function([[1]])\n",
    "# dec.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0f787d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predict=lin_clf.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dcb1ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c09eeca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6190681726396011, 0.4245606368594977, 0.4416240815874178, None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(test_integer_encoded, svm_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "950ef400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5543478260869565, 0.5543478260869565, 0.5543478260869565, None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(test_integer_encoded, svm_predict, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e191c772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5860481352523278, 0.5543478260869565, 0.538704870480898, None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(test_integer_encoded, svm_predict, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7abb2a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5625    , 0.52403846, 0.32432432, 0.60942761, 1.        ,\n",
       "        0.88461538, 0.42857143]),\n",
       " array([0.16071429, 0.73154362, 0.43636364, 0.61355932, 0.08      ,\n",
       "        0.58974359, 0.36      ]),\n",
       " array([0.25      , 0.61064426, 0.37209302, 0.61148649, 0.14814815,\n",
       "        0.70769231, 0.39130435]),\n",
       " array([ 56, 149,  55, 295,  25,  39,  25], dtype=int64))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(test_integer_encoded, svm_predict, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "44e415c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "simpleNN = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,50, 7), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aaf6c25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I515580\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\I515580\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(100, 50, 7), random_state=1,\n",
       "              solver='lbfgs')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleNN.fit(sentence_embeddings, integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5c29089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleNN_predict=simpleNN.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6f122ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4372340451632564, 0.38563614343632874, 0.39308916785033937, None)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(test_integer_encoded, simpleNN_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5f4a4f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from tensorflow_hub) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from tensorflow_hub) (1.20.3)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\i515580\\appdata\\local\\continuum\\anaconda3_1\\envs\\demo\\lib\\site-packages (from protobuf>=3.8.0->tensorflow_hub) (1.12.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7ab86a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e528bff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028E9C88D5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028E9C88D5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028E9BC9C168> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028E9BC9C168> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028E8A71B558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028E8A71B558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "module_path=r'C:\\Users\\I515580\\Documents\\universal-sentence-encoder_4'\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.load(module_path)\n",
    "# with tf.compat.v1.Session() as session:\n",
    "\n",
    "#   session.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "387c8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_embeddings_usc = embed(sentences)\n",
    "test_embeddings_usc = embed(test_df['sentence'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "250f3a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I515580\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5065696185917783, 0.43966280457277745, 0.4348419063969498, None)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_clf_usc = svm.LinearSVC()\n",
    "lin_clf_usc.fit(sentences_embeddings_usc, integer_encoded)\n",
    "svm_predict_usc=lin_clf_usc.predict(test_embeddings_usc)\n",
    "precision_recall_fscore_support(test_integer_encoded, svm_predict_usc, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a9c93302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I515580\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028E8A9B6DC8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028E8A9B6DC8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "C:\\Users\\I515580\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4516000640641776, 0.4480301576264168, 0.43807769865115515, None)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleNN_usc = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,50, 7), random_state=1)\n",
    "simpleNN_usc.fit(sentences_embeddings_usc, integer_encoded)\n",
    "simpleNN_predict_usc=simpleNN_usc.predict(test_embeddings_usc)\n",
    "precision_recall_fscore_support(test_integer_encoded, simpleNN_predict_usc, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56376892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
